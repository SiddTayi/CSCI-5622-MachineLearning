---
title: "ML-Exam 2"
author: "Siddharth Tayi"
date: "2023-12-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(arm)
library(arules)
library(arulesViz)
library(viridis)
library(arules)
library(TSP)
library(data.table)
library(ggplot2)
library(Matrix)
library(tcltk)
library(dplyr)
library(devtools)
library(purrr)
library(tidyr)
library(stopwords)
library(tokenizers)

```

```{r}

setwd("D:/Masters-2023/Machine Learning/ML - Exam 2")
data = read.csv('ML_EXAM2_2.csv')
head(data)

```

```{r}

prods = data$products_viewed

df = data.frame(prods = prods)

head(df)

```

```{r}

# Create an empty csv file to store the tokenized comments
transaction_comments = "product_tokenized.csv"

# Open the file
trans <- file(transaction_comments)

# Tokenize the text in the "lyrics" column
Tokens <- tokenizers::tokenize_words(
  df$prods[1], stopwords = stopwords::stopwords("en"), 
  lowercase = T,  strip_punct = T, strip_numeric = T,
  simplify = T)

# Words to be removed
words_to_remove <- c("fuck", "bitch", "shit", "hell", "n", "que", "de", "en", "la", "el", "canci", "porn", "el",	"que", "est", "leyendo", "este", "comentario",	"pasen",	"por",	"mi", "canal",	"que",	"estar",	"subiendo",	"mucho",	"contenidos",	"gracias",	"por",	"detenerte",	"leer",	"este",	"comentario",	"que",	"dios",	'te',	"bendiga", "fucked"
)

# Remove specified words from tokens
Tokens <- Tokens[!Tokens %in% words_to_remove]

# Write tokens (excluding specified words) to the file. It concatenates the tokens into a single string, separates them by commas, and writes them to the file.
#cat(paste(unlist(Tokens), collapse = ","), "\n", file = Trans)

# Append remaining lists of tokens into file
Trans <- file(transaction_comments, open = "a")
for (i in 2:nrow(df)) {
  Tokens <- tokenize_words(df$prods[i],
                           stopwords = stopwords::stopwords("en"),
                           lowercase = TRUE,
                           strip_punct = TRUE,
                           simplify = TRUE)
  
  # Remove specified words from tokens
  Tokens <- Tokens[!Tokens %in% words_to_remove]
  
  # Write tokens (excluding specified words) to the file
  cat(paste(unlist(Tokens), collapse = ","), "\n", file = Trans)
  #cat(unlist(Tokens))
}

close(Trans) # Close the file

print("Done processing")



```

```{r}

# Read the CSV file
prod_df <- read.csv('product_tokenized.csv', header = FALSE, sep = ",")
head(prod_df)

# Convert all columns to character
prod_df <- prod_df %>%
  mutate_all(as.character)

# Check the structure of the dataframe
str(prod_df)

# Remove the first row (assuming it contains header information)
prod_df <- prod_df[-1, ]
head(prod_df)


```

```{r}

capabilities()["tcltk"]
library(arulesViz)

```

```{r}

prod_bask = read.transactions("product_tokenized.csv",
              
                           rm.duplicates = FALSE, 
                           format = "basket",  
                           sep="," #) 
                           ,cols=1
                           ) 


```

```{r}

associa_rules <- apriori(data = prod_bask, 
                        parameter = list(support = 0.086, 
                                         confidence = 0.1, minlen = 2))


```

```{r}


# Visualising the results
supp <- inspect(sort(associa_rules, by = 'support')[1:5])
conf <- inspect(sort(associa_rules, by = 'confidence')[1:5])
lift <- inspect(sort(associa_rules, by = 'lift')[1:5])

```

```{r}
# Plot
itemFrequencyPlot(prod_bask, topN = 15,
                  col = 'lightblue',
                  main = 'Relative Item Frequency Plot')
```

```{r}

plot(associa_rules, 
     method = "graph", 
     measure = "confidence", 
     shading = "lift")

```

```{r}
# Filter rules based on support, confidence, or lift
filtered_rules <- subset(associa_rules, lift > 1)

# Inspect the top rules
top_rules <- inspect(sort(filtered_rules, by = 'support')[1:5])

# Print the top rules
print(top_rules)
```

```{r}
BreadRules <- apriori(data=prod_bask,parameter = list(supp=.001, conf=.01, minlen=2),
                       appearance = list(lhs="shirt"),
                       control=list(verbose=FALSE))
BreadRules <- sort(BreadRules, decreasing=TRUE, by="confidence")
inspect(BreadRules)

```
